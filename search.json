[{"title":"在软渲染器中实现法线贴图 (Normal Mapping)","path":"/2025/04/12/NORMAL-MAPPING/","content":"在实时计算机图形学中，模型的细节往往受到多边形数量的限制。为了在不显著增加模型复杂度的前提下，模拟出丰富的表面细节（如凹凸、划痕、纹理），法线贴图技术应运而生。本文将详细介绍如何在基于 C++ 的软件渲染器中实现切线空间法线贴图 (Tangent Space Normal Mapping)。 1. 问题的提出：低模的局限性传统的低多边形模型 (Low-Poly Model) 依赖于顶点法线 (Vertex Normals) 进行光照计算。通过 Gouraud Shading 或 Phong Shading，我们可以在顶点之间插值法线，获得平滑的光照过渡效果。然而，这种方法无法表现模型表面的微小几何细节。如果想要模型拥有丰富的凹凸细节，就需要极高数量的多边形，这对于实时渲染来说通常是不可接受的。 （示意图：左侧为低模+顶点法线光照，右侧为低模+法线贴图光照） 2. 解决方案：法线贴图法线贴图的核心思想是：用一张纹理来存储模型表面各点的法线信息。这张特殊的纹理被称为“法线贴图”。在渲染时，我们不再直接使用插值得到的顶点法线，而是从法线贴图中采样对应片元 (Fragment) 的法线向量，并用这个采样得到的法线来进行光照计算。 由于纹理可以存储非常丰富的信息，即使模型本身多边形数量很少，通过法线贴图也能模拟出极其逼真的表面细节。 3. 关键概念：切线空间 (Tangent Space)直接将世界空间 (World Space) 或模型空间 (Model Space) 的法线存储在纹理中是可行的，但这会导致法线贴图与模型的特定姿态或变换绑定，难以复用。更常用的方法是使用 切线空间 (Tangent Space)。 切线空间是一个局部坐标系，定义在模型的每个表面点上。它由三个相互正交（或近似正交）的基向量构成： 法线 (Normal - N): 即该点的原始顶点法线，通常垂直于表面。 切线 (Tangent - T): 平行于表面，通常沿着纹理坐标 U 的增加方向。 副切线 (Bitangent - B): 平行于表面，通常沿着纹理坐标 V 的增加方向，并且可以通过 N 和 T 的叉乘得到 (B = cross(N, T)) 来保证正交性。 （示意图：模型表面一点的切线空间 TBN 基向量） 法线贴图中存储的是相对于这个局部 TBN 坐标系的法线扰动。通常，RGB 通道对应 TBN 向量： R -&gt; Tangent 方向分量 G -&gt; Bitangent 方向分量 B -&gt; Normal 方向分量 一个“平坦”表面的法线在切线空间中通常是 (0, 0, 1)。由于颜色通道通常存储在 [0, 1] 范围内，而法线分量在 [-1, 1] 范围内，因此需要进行映射。常用的映射方式为： 存储值 &#x3D; (法线分量 + 1.0) &#x2F; 2.0 或者反过来，从纹理采样值恢复法线分量： 法线分量 &#x3D; 采样值 * 2.0 - 1.0 因此，法线贴图中常见的“基准”蓝色 (0.5, 0.5, 1.0) 就代表了切线空间中的 (0, 0, 1) 法线，即未发生扰动的原始表面法线方向。 使用切线空间的好处： 解耦: 法线信息与模型的具体旋转、变形无关。 复用: 同一张法线贴图可以应用在不同模型或模型的不同部分（只要它们的 UV 布局允许）。 压缩友好: 大部分法线的 Z 分量（Normal 方向）都接近 1，可以通过优化存储。 4. 实现步骤要在我们的软渲染器中实现切线空间法线贴图，需要修改渲染管线的多个阶段。 4.1 计算顶点切线和副切线我们需要为模型的每个顶点计算其 TBN 基础向量。这通常在模型加载后、渲染前完成。计算方法基于构成三角形的顶点位置和纹理坐标： 对于三角形 P0, P1, P2 及其对应的纹理坐标 UV0, UV1, UV2： 计算边向量： Edge1 = P1 - P0 Edge2 = P2 - P0 计算 UV 差量： DeltaUV1 = UV1 - UV0 DeltaUV2 = UV2 - UV0 计算系数 f： f = 1.0 / (DeltaUV1.x * DeltaUV2.y - DeltaUV2.x * DeltaUV1.y) 计算切线 T 和副切线 B： Tangent.x = f * (DeltaUV2.y * Edge1.x - DeltaUV1.y * Edge2.x) Tangent.y = f * (DeltaUV2.y * Edge1.y - DeltaUV1.y * Edge2.y) Tangent.z = f * (DeltaUV2.y * Edge1.z - DeltaUV1.y * Edge2.z) Bitangent.x = f * (-DeltaUV2.x * Edge1.x + DeltaUV1.x * Edge2.x) Bitangent.y = f * (-DeltaUV2.x * Edge1.y + deltaUV1.x * Edge2.y) Bitangent.z = f * (-DeltaUV2.x * Edge1.z + deltaUV1.x * Edge2.z) 计算出的 T 和 B 需要累加到每个顶点上（因为一个顶点可能被多个三角形共享）。 最后，对每个顶点的 T 和 B 进行正交化和归一化处理，常用 Gram-Schmidt 方法： T = normalize(T - N * dot(N, T)) &#x2F;&#x2F; 使 T 正交于 N 检查 dot(cross(N, T), B) 的符号，判断 TBN 坐标系的左右手性是否与 UV 坐标系一致，必要时翻转 T。 B = normalize(cross(N, T)) &#x2F;&#x2F; 重新计算 B 以确保正交 代码片段 (Model::calculateTangents): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// src/core/model.cppvoid Model::calculateTangents() &#123; tangents.assign(numVertices(), vec3f(0.0f, 0.0f, 0.0f)); bitangents.assign(numVertices(), vec3f(0.0f, 0.0f, 0.0f)); // --- Loop through faces to calculate T and B contributions --- for (size_t i = 0; i &lt; numFaces(); ++i) &#123; // ... (Get vertices v0, v1, v2 and uvs uv0, uv1, uv2) ... vec3f edge1 = v1 - v0; vec3f edge2 = v2 - v0; vec2f deltaUV1 = uv1 - uv0; vec2f deltaUV2 = uv2 - uv0; float f = 1.0f / (deltaUV1.x * deltaUV2.y - deltaUV2.x * deltaUV1.y); if (std::isinf(f) || std::isnan(f)) &#123; f = 0.0f; &#125; // Avoid NaN/Inf vec3f tangent = (edge1 * deltaUV2.y - edge2 * deltaUV1.y) * f; vec3f bitangent = (edge2 * deltaUV1.x - edge1 * deltaUV2.x) * f; // Accumulate for vertices for (int j = 0; j &lt; 3; ++j) &#123; tangents[face.vertIndex[j]] = tangents[face.vertIndex[j]] + tangent; bitangents[face.vertIndex[j]] = bitangents[face.vertIndex[j]] + bitangent; &#125; &#125; // --- Loop through vertices to orthogonalize and normalize --- for (size_t i = 0; i &lt; numVertices(); ++i) &#123; const vec3f&amp; n = getNormal(i); // Assuming normal indices match vertex indices after processing vec3f&amp; t = tangents[i]; vec3f&amp; b = bitangents[i]; if (t.length() &gt; 1e-6f &amp;&amp; n.length() &gt; 1e-6f) &#123; // Gram-Schmidt orthogonalize T against N t = (t - n * n.dot(t)).normalized(); // Check handedness and recalculate B if (n.cross(t).dot(b) &lt; 0.0f) &#123; t = t * -1.0f; // Flip tangent if needed &#125; b = n.cross(t).normalized(); // Ensure B is orthogonal and normalized &#125; else &#123; // Handle degenerate cases: Create arbitrary orthogonal basis vec3f up = (std::abs(n.y) &lt; 0.99f) ? vec3f(0.0f, 1.0f, 0.0f) : vec3f(1.0f, 0.0f, 0.0f); t = n.cross(up).normalized(); b = n.cross(t).normalized(); &#125; // Fallback for NaN/Inf safety if (std::isnan(t.x) || std::isinf(t.x)) t = vec3f(1,0,0); if (std::isnan(b.x) || std::isinf(b.x)) b = vec3f(0,0,1); &#125;&#125; 将计算得到的 tangents 和 bitangents 存储在 Model 类中。 4.2 数据准备与传递 Material: 在 Material 结构体中添加 normalTexture 成员及加载方法。 Shader Uniforms: 在 Shader 基类中添加 uniform_NormalTexture (类型 Texture) 和 uniform_UseNormalMap (类型 bool)。 Vertex Input: 修改 VertexInput 结构体，添加 tangent 和 bitangent 成员。 12345678// include/core/shader.hstruct VertexInput &#123; vec3f position; vec3f normal; vec2f uv; vec3f tangent; // Added vec3f bitangent; // Added&#125;; Varyings: 修改 Varyings 结构体，传递世界空间下的 TBN 基向量。 12345678910// include/core/shader.hstruct Varyings &#123; vec4f clipPosition; vec3f worldPosition; vec2f uv; // World-space TBN basis vectors vec3f tangent; // World Tangent vec3f bitangent; // World Bitangent vec3f normal; // World (Geometric) Normal&#125;; Renderer: 在 Renderer::drawModel 中，设置 uniform_NormalTexture 和 uniform_UseNormalMap。在构建 VertexInput 时，从 Model 获取 tangent 和 bitangent。 4.3 顶点着色器 (Vertex Shader)顶点着色器的主要任务是将 TBN 基向量从模型空间转换到世界空间，并传递给片元着色器。 代码片段 (BlinnPhongShader::vertex): 123456789101112131415161718192021222324252627// src/core/blinn_phong_shader.cppVaryings BlinnPhongShader::vertex(const VertexInput&amp; input) &#123; Varyings output; vec4f modelPos4(input.position, 1.0f); vec4f modelNormal4(input.normal, 0.0f); vec4f modelTangent4(input.tangent, 0.0f); vec4f modelBitangent4(input.bitangent, 0.0f); // Calculate world position output.worldPosition = (uniform_ModelMatrix * modelPos4).xyz(); // Transform TBN vectors to world space using Normal Matrix // uniform_NormalMatrix is typically transpose(inverse(ModelMatrix)) // Ensure they are normalized after transformation. output.normal = (uniform_NormalMatrix * modelNormal4).xyz().normalized(); output.tangent = (uniform_NormalMatrix * modelTangent4).xyz().normalized(); output.bitangent = (uniform_NormalMatrix * modelBitangent4).xyz().normalized(); // Optional: Recalculate bitangent worldB = cross(worldN, worldT) here for robustness. // Pass UVs output.uv = input.uv; // Calculate clip space position output.clipPosition = uniform_MVP * modelPos4; return output;&#125; 4.4 片元着色器 (Fragment Shader)片元着色器是实现法线贴图的核心： 检查是否使用法线贴图: 根据 法线的 texture 是否为空为标志。 采样法线贴图: 如果使用，则根据插值得到的 uv 坐标采样 uniform_NormalTexture。 解压法线: 将采样到的 [0, 1] 颜色值转换回 [-1, 1] 的切线空间法线向量 N_{tangent}。 N_{tangent} &#x3D; normalize(Sample_{RGB} * 2.0 - 1.0) 构建 TBN 矩阵: 使用从顶点着色器插值得到的世界空间 TBN 基向量（需要重新归一化）。 T &#x3D; normalize(input.tangent) B &#x3D; normalize(input.bitangent) N_{geom} &#x3D; normalize(input.normal) 转换法线: 将切线空间法线 N_{tangent} 转换到世界空间。 N_{world} &#x3D; normalize(T * N_{tangent}.x + B * N_{tangent}.y + N_{geom} * N_{tangent}.z) 光照计算: 使用计算得到的 N_{world} (如果使用了法线贴图) 或 N_{geom} (如果未使用) 进行后续的 Blinn-Phong 或其他光照模型计算。 代码片段 (BlinnPhongShader::fragment): 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// src/core/blinn_phong_shader.cppbool BlinnPhongShader::fragment(const Varyings&amp; input, vec3f&amp; outColor) &#123; vec3f N; // The final normal used for lighting if (uniform_UseNormalMap &amp;&amp; !uniform_NormalTexture.empty()) &#123; // 1. Sample the normal map vec3f tangentNormalSample = uniform_NormalTexture.sample(input.uv.x, input.uv.y); // 2. Unpack from [0,1] to [-1,1] and normalize vec3f tangentNormal = (tangentNormalSample * 2.0f) - vec3f(1.0f, 1.0f, 1.0f); tangentNormal = tangentNormal.normalized(); // Ensure unit length // 3. Get interpolated world-space TBN basis (renormalize) vec3f T = input.tangent.normalized(); vec3f B = input.bitangent.normalized(); vec3f N_geom = input.normal.normalized(); // 4. Transform tangent-space normal to world space // N_world = T*Nx_tan + B*Ny_tan + N_geom*Nz_tan N = T * tangentNormal.x + B * tangentNormal.y + N_geom * tangentNormal.z; N = N.normalized(); // Final normal for lighting &#125; else &#123; // Use interpolated geometric normal if no normal map N = input.normal.normalized(); &#125; // --- Proceed with Blinn-Phong lighting using the final Normal N --- vec3f V = (uniform_CameraPosition - input.worldPosition).normalized(); // View direction vec3f totalColor = uniform_AmbientLight * uniform_AmbientColor; // Start with ambient // Get material properties (potentially textured) vec3f matDiffuse = uniform_DiffuseColor; if (!uniform_DiffuseTexture.empty()) &#123; matDiffuse = matDiffuse * uniform_DiffuseTexture.sample(input.uv.x, input.uv.y); &#125; // ... (Get matSpecular, matShininess) ... for (const auto&amp; light : uniform_Lights) &#123; // ... (Calculate L, lightCol, attenuation) ... // Diffuse float NdotL = std::max(0.0f, N.dot(L)); vec3f diffuse = matDiffuse * lightCol * NdotL * attenuation; // Specular (Blinn-Phong) vec3f H = (L + V).normalized(); float NdotH = std::max(0.0f, N.dot(H)); float specFactor = fastPow(NdotH, uniform_Shininess); // Use the fastPow utility vec3f specular = uniform_SpecularColor * lightCol * specFactor * attenuation; totalColor = totalColor + diffuse + specular; &#125; // Clamp final color outColor.x = std::min(1.0f, std::max(0.0f, totalColor.x)); outColor.y = std::min(1.0f, std::max(0.0f, totalColor.y)); outColor.z = std::min(1.0f, std::max(0.0f, totalColor.z)); return true; // Pixel should be written&#125; 4.5 插值 (Interpolation)确保 Renderer::interpolateVaryings 函数能够正确地对新增的 tangent, bitangent, normal 向量进行透视矫正插值。由于 interpolateVaryings 内部使用了模板化的 perspectiveCorrectInterpolate，只需要在 interpolateVaryings 中添加对这三个新成员的调用即可。 123456789// src/core/renderer.cppVaryings Renderer::interpolateVaryings(float t, const Varyings&amp; start, const Varyings&amp; end, float startInvW, float endInvW) const &#123; Varyings result; // ... (Interpolate worldPosition, uv) ... result.normal = perspectiveCorrectInterpolate(t, start.normal, end.normal, startInvW, endInvW); result.tangent = perspectiveCorrectInterpolate(t, start.tangent, end.tangent, startInvW, endInvW); result.bitangent = perspectiveCorrectInterpolate(t, start.bitangent, end.bitangent, startInvW, endInvW); return result;&#125; 5. 总结与效果通过以上步骤，我们就成功地在软渲染器中集成了切线空间法线贴图。渲染低多边形模型时，通过在片元着色器中查询法线贴图并使用得到的法线进行光照计算，可以在几乎不增加几何复杂度的前提下，极大地提升模型的表面细节和真实感。 这项技术是现代实时渲染中不可或缺的一部分，能够以较低的性能开销实现高质量的视觉效果。 6. 注意事项 Tangent Calculation: 上述切线计算方法比较基础，对于复杂的 UV 布局或重叠 UV 可能产生问题。更精确的方法（如 MikkTSpace）更为健壮。 Normal Map Format: 注意法线贴图的 Y 分量（通常是绿色通道）在不同规范（如 OpenGL 和 DirectX）中可能方向相反。需要确保加载和解压时使用正确的约定。 TBN 正交性: 插值后的 TBN 基向量可能不再严格正交，在片元着色器中重新正交化（如通过 B &#x3D; cross(N, T)）可以提高精度，但会增加计算量。 sRGB: 如果法线贴图被错误地当作 sRGB 纹理处理，会导致解压出的法线不准确。应确保法线贴图作为线性数据处理。","tags":["C++","图形学","渲染","法线贴图","软渲染器"],"categories":["技术分享"]},{"title":"Blinn-Phong 着色器实现","path":"/2025/04/06/BLINN-PHONG-SHADER/","content":"Blinn-Phong 着色器实现概述Blinn-Phong 着色模型是经典 Phong 模型的改进版本，通过引入半角向量(Halfway Vector)优化了高光计算。我们的实现包含完整的顶点和片段着色器处理流程。 核心实现1. 顶点着色器1234567891011121314151617Varyings BlinnPhongShader::vertex(const VertexInput&amp; input) &#123; Varyings output; // 计算世界空间位置 vec4f worldPos4 = uniform_ModelMatrix * vec4f(input.position, 1.0f); output.worldPosition = Vector3&lt;float&gt;(worldPos4.x, worldPos4.y, worldPos4.z); // 变换法线到世界空间 vec4f worldNormal4 = uniform_NormalMatrix * vec4f(input.normal, 0.0f); output.worldNormal = worldNormal4.xyz().normalized(); // 传递UV坐标 output.uv = input.uv; // 计算裁剪空间位置 output.clipPosition = uniform_MVP * vec4f(input.position, 1.0f); return output;&#125; 2. 片段着色器片段着色器实现了完整的 Blinn-Phong 光照模型： 12345678910111213141516171819202122232425262728293031323334bool BlinnPhongShader::fragment(const Varyings&amp; input, Vector3&lt;float&gt;&amp; outColor) &#123; // 标准化法线和视线方向 Vector3&lt;float&gt; N = input.worldNormal.normalized(); Vector3&lt;float&gt; V = (uniform_CameraPosition - input.worldPosition).normalized(); // 材质属性 Vector3&lt;float&gt; matDiffuse = uniform_Material.diffuseColor; if (uniform_Material.hasDiffuseTexture()) &#123; matDiffuse = matDiffuse * uniform_Material.diffuseTexture.sample(input.uv.x, input.uv.y); &#125; // ...其他材质属性处理 // 光照计算 Vector3&lt;float&gt; totalColor = uniform_AmbientLight * uniform_Material.ambientColor; for (const auto&amp; light : uniform_Lights) &#123; // 计算光线方向 Vector3&lt;float&gt; L = light.getDirectionTo(input.worldPosition); // 漫反射计算 float diffFactor = std::max(0.0f, N.dot(L)); Vector3&lt;float&gt; diffuse = matDiffuse * light.color * diffFactor; // Blinn-Phong 高光计算 Vector3&lt;float&gt; H = (L + V).normalized(); float specFactor = fastPow(std::max(0.0f, N.dot(H)), matShininess); Vector3&lt;float&gt; specular = matSpecular * light.color * specFactor; totalColor += diffuse + specular; &#125; // 颜色钳制 outColor = totalColor.clamp(0.0f, 1.0f); return true;&#125; 关键技术点1. 快速幂计算1234567891011template &lt;typename T&gt;T fastPow(T base, int n) &#123; // 使用快速幂算法优化高光计算 T res = static_cast&lt;T&gt;(1); while (n) &#123; if (n &amp; 1) res = res * base; base = base * base; n &gt;&gt;= 1; &#125; return res;&#125; 2. 光照类型支持 方向光(Directional Light) 点光源(Point Light) 环境光(Ambient Light) 3. 材质系统 漫反射颜色&#x2F;贴图 高光颜色 光泽度(Shininess) 环境光反射率 使用方法 创建 BlinnPhongShader 实例 设置必要的 uniform 变量: 模型、视图、投影矩阵 材质属性 光源参数 绑定到渲染器使用 性能优化 使用快速幂算法优化高光计算 提前终止无效的光照计算 向量运算的规范化处理 后续改进计划 添加法线贴图支持 实现 PBR 材质系统 支持多光源阴影计算","tags":["计算机图形学","C++","渲染引擎","着色器"],"categories":["技术实践"]},{"title":"SoftRasterizer 渲染流程解析","path":"/2025/04/06/RENDERING-PIPELINE/","content":"SoftRasterizer 渲染流程解析概述本文档详细分析 SoftRasterizer 的渲染管线实现，涵盖从模型加载到最终像素输出的完整流程。渲染管线主要分为初始化阶段和每帧渲染阶段。 核心渲染流程1. 初始化阶段1234567891011121314// main.cpp 中的初始化代码Framebuffer framebuffer(width, height);Model model;model.loadFromObj(&quot;resources/obj/african_head.obj&quot;);model.loadDiffuseTexture(&quot;resources/diffuse/african_head_diffuse.tga&quot;);Camera camera(Vector3&lt;float&gt;(0, 1, 3), Vector3&lt;float&gt;(0, 0, 0), Vector3&lt;float&gt;(0, 1, 0));camera.setPerspective(45.0f, aspectRatio, near, far);Renderer renderer(framebuffer, camera);std::vector&lt;Light&gt; lights;// 设置光源...auto shader = std::make_shared&lt;BlinnPhongShader&gt;();renderer.setShader(shader); 2. 每帧渲染阶段2.1 设置Uniform变量123456789// renderer.cpp - drawModel()currentShader-&gt;uniform_ModelMatrix = modelMatrix;currentShader-&gt;uniform_ViewMatrix = viewMatrix; currentShader-&gt;uniform_ProjectionMatrix = projectionMatrix;currentShader-&gt;uniform_MVP = projectionMatrix * viewMatrix * modelMatrix;currentShader-&gt;uniform_NormalMatrix = modelMatrix.inverse().transpose();currentShader-&gt;uniform_CameraPosition = cameraPosition;currentShader-&gt;uniform_Lights = lights;currentShader-&gt;uniform_Material = material; 2.2 顶点处理12345678// shader.hvirtual Varyings vertex(const VertexInput&amp; input) = 0;// 顶点着色器处理流程：// 1. 将顶点位置变换到世界空间// 2. 变换法线到世界空间// 3. 计算裁剪空间位置// 4. 传递UV等属性 2.3 三角形组装与光栅化123456789101112// renderer.cpp// 1. 背面剔除float signedArea = (p1.x - p0.x) * (p2.y - p0.y) - (p2.x - p0.x) * (p1.y - p0.y);if (signedArea &lt; 0) continue;// 2. 扫描线光栅化for (int y = yStart; y &lt;= yEnd; ++y) &#123; // 沿三角形两边插值 // 在扫描线内插值片段属性 // 深度测试 if (depth &gt;= framebuffer.getDepth(x, y)) continue;&#125; 2.4 片段处理12345678// shader.h virtual bool fragment(const Varyings&amp; input, Vector3&lt;float&gt;&amp; outColor) = 0;// 片段着色器处理流程：// 1. 标准化法线和视线方向// 2. 采样纹理(如果有)// 3. 计算光照(漫反射+高光)// 4. 输出最终颜色 2.5 帧缓冲更新1framebuffer.setPixel(x, y, fragmentColor, depth); 关键技术点1. 透视校正插值1234567Varyings Renderer::interpolateVaryings(float t, const Varyings&amp; start, const Varyings&amp; end, float startInvW, float endInvW) &#123; // 使用1/w进行透视校正插值 float currentInvW = startInvW + (endInvW - startInvW) * t; float currentW = 1.0f / currentInvW; // 对每个属性进行插值&#125; 2. 深度缓冲12345// 深度值映射到[0,1]范围screenVertices[j].z = (ndcPos.z + 1.0f) * 0.5f; // 深度测试if (depth &gt;= framebuffer.getDepth(x, y)) continue; 3. 光照计算优化1234567891011// 使用快速幂算法优化高光计算template &lt;typename T&gt;T fastPow(T base, int n) &#123; T res = static_cast&lt;T&gt;(1); while (n) &#123; if (n &amp; 1) res = res * base; base = base * base; n &gt;&gt;= 1; &#125; return res;&#125; 渲染管线图示12345678910graph TD A[模型加载] --&gt; B[设置相机和光源] B --&gt; C[设置着色器和材质] C --&gt; D[顶点处理] D --&gt; E[三角形组装] E --&gt; F[光栅化] F --&gt; G[片段处理] G --&gt; H[深度测试] H --&gt; I[帧缓冲更新] I --&gt; J[输出图像] 性能优化 提前深度测试：在片段着色器前进行深度测试 背面剔除：减少约50%的三角形处理 快速幂算法：优化高光计算 透视校正插值：保证纹理和属性正确插值 后续改进计划 实现法线贴图支持 添加阴影计算 支持延迟渲染管线 实现多线程渲染","tags":["计算机图形学","C++","渲染引擎","渲染管线"],"categories":["技术实践"]},{"title":"Perspective Projection","path":"/2025/04/05/PERSPECTIVE-PROJECTION/","content":"实现软光栅化中的透视投影：从基础渲染到深度测试优化在开发软光栅化渲染器时，透视投影是实现真实感渲染的关键一步。本文基于一次代码修改（git diff），详细讲述如何将一个基础的模型渲染系统升级为支持透视投影的渲染管线，包括矩阵变换、深度处理和透视校正的实现过程。 背景最初的渲染代码（src/main.cpp）使用简单的屏幕空间投影，直接将模型的顶点映射到帧缓冲区，没有考虑透视效果和深度缓冲的正确性： 1model.renderSolid(framebuffer, vec3f(1.0f, 1.0f, 1.0f), vec3f(0.0f, 0.0f, 1.0f)); 目标是引入透视投影，使远处的物体变小，并通过深度测试实现正确的遮挡关系。以下是实现过程的步骤。 步骤 1：引入变换矩阵在 src/main.cpp 中，我们添加了模型、视图和投影矩阵，用于将顶点从模型空间变换到裁剪空间： 12345678910111213float near = 0.1f;float far = 100.0f;mat4 modelMatrix = mat4::identity();mat4 viewMatrix = mat4::translation(0, 0, -3); // 相机后移mat4 projectionMatrix = mat4::perspective( 45.0f * 3.1415926f / 180.0f, // FOV (float)width/height, // 宽高比 near, // 近裁剪面 far // 远裁剪面);mat4 mvp = projectionMatrix * viewMatrix * modelMatrix; 模型矩阵：保持不变（identity），后续可添加旋转或缩放。 视图矩阵：将相机向后移动 3 个单位，模拟观察者的位置。 投影矩阵：使用透视投影，定义视锥体（FOV 为 45°）。 MVP 矩阵：组合三者，用于顶点变换。 渲染调用改为： 1model.renderSolid(framebuffer, near, far, mvp, vec3f(1.0f, 1.0f, 1.0f), vec3f(0.0f, 0.0f, 1.0f)); 步骤 2：顶点变换与透视除法在 src/core/model.cpp 中，renderSolid 方法从简单的屏幕映射升级为完整的透视投影管线： 2.1 顶点变换将顶点从模型空间变换到裁剪空间： 12345678910vec4f clip_coords[3];vec3f world_coords[3];float w_values[3];for (int j = 0; j &lt; 3; j++) &#123; world_coords[j] = vertices[face[j]]; vec4f v(world_coords[j], 1.0f); clip_coords[j] = mvp * v; w_values[j] = clip_coords[j].w;&#125; 使用齐次坐标（w&#x3D;1）进行矩阵乘法。 存储 w 值，用于后续透视除法和校正。 2.2 简单裁剪检查丢弃完全在近裁剪面外的三角形： 12345if (clip_coords[0].z &lt; -w_values[0] &amp;&amp; clip_coords[1].z &lt; -w_values[1] &amp;&amp; clip_coords[2].z &lt; -w_values[2]) &#123; continue;&#125; 2.3 透视除法与视口变换将裁剪空间坐标转换为 NDC（标准化设备坐标），并映射到屏幕空间： 12345678910111213141516Vertex vertices[3];for (int j = 0; j &lt; 3; j++) &#123; if (w_values[j] &lt;= 0) continue; float invW = 1.0f / w_values[j]; vec3f ndc( clip_coords[j].x * invW, clip_coords[j].y * invW, clip_coords[j].z * invW ); vertices[j].x = (ndc.x + 1.0f) * fb.width * 0.5f; vertices[j].y = (ndc.y + 1.0f) * fb.height * 0.5f; // ... 深度映射 ... vertices[j].u = tex_coords[j].x * invW; vertices[j].v = tex_coords[j].y * invW; vertices[j].w = invW;&#125; 透视除法：除以 w 得到 NDC。 视口变换：将 [-1,1] 范围映射到屏幕坐标。 步骤 3：深度处理优化3.1 深度值映射将视空间的 z 值映射到 [0,1] 范围，靠近相机为 0，远离为 1： 123456float zEye = clip_coords[j].z;if (w_values[j] != 0) &#123; vertices[j].z = (1.0f - (near * far / zEye * invW + near) / (far - near)) * 0.5f + 0.5f;&#125; else &#123; vertices[j].z = 1.0f;&#125; 使用非线性映射，确保透视效果下的深度分布正确。 反转逻辑，使更近的点得到更小的深度值。 3.2 深度测试调整在 src/core/framebuffer.cpp 中，将深度测试改为 “小于” 测试： 1234if (depth &lt; zBuffer[index]) &#123; // z值越小表示越近 zBuffer[index] = depth; pixels[index] = color;&#125; 初始化时将深度缓冲区清为最大值： 1234Framebuffer::Framebuffer(int w, int h) : width(w), height(h), pixels(w * h), zBuffer(w * h, std::numeric_limits&lt;float&gt;::max()) &#123;&#125;void Framebuffer::clearZBuffer() &#123; std::fill(zBuffer.begin(), zBuffer.end(), std::numeric_limits&lt;float&gt;::max());&#125; 步骤 4：透视校正插值在 drawScanlines 中添加透视校正插值，确保纹理随深度正确变化： 12345678910float wa = interpolate&lt;float, int&gt;(vStartA.w, vStartA.y, vEndA.w, vEndA.y, y);float wb = interpolate&lt;float, int&gt;(vStartB.w, vStartB.y, vEndB.w, vEndB.y, y);// ...float w = wa + (wb - wa) * t;if (useTexture &amp;&amp; w != 0) &#123; float invW = 1.0f / w; float u = (ua + (ub - ua) * t) * invW; float v = (va + (vb - va) * t) * invW; finalColor = texture.sample(u, v) * color;&#125; 插值 1&#x2F;w 而不是直接插值纹理坐标。 在最终采样前除以 w，实现透视校正。 成果与反思通过以上步骤，我们实现了： 透视投影：物体随距离变小。 深度测试：靠近相机的物体遮挡远处的物体。 纹理校正：纹理随视角正确变形。 然而，这仍是一个简化实现。未来的改进可以包括： 更复杂的裁剪算法（处理跨越裁剪面的三角形）。 支持透视投影下的背面剔除。 优化性能（如 SIMD 加速）。 代码已成功渲染出带有透视效果的非洲人头模型，保存为 output.tga。这是一个软光栅化学习过程中的重要里程碑！","tags":["C++","Rendering","Perspective Projection"],"categories":["Graphics","Soft Rasterizer"]},{"title":"Camera Implementation","path":"/2025/04/05/CAMERA/","content":"概述本文档详细记录了在 SoftRasterizer 项目中实现 Camera 类的全过程，包括其设计、核心代码、应用方式以及验证方法。通过该类，我们实现了灵活的相机控制，支持 OpenGL 风格的视图变换。 核心实现1. Camera 类定义1234567891011121314151617181920// camera.h#pragma once#include &quot;math/matrix.h&quot;#include &quot;math/vector.h&quot;class Camera &#123;public: Camera(const vec3f&amp; position, const vec3f&amp; target, const vec3f&amp; up); void setPerspective(float fovDegrees, float aspectRatio, float near, float far); mat4 getMVP(const mat4&amp; modelMatrix) const; void setPosition(const vec3f&amp; position);private: vec3f m_position; // 相机位置 vec3f m_target; // 目标点 vec3f m_up; // 上方向 mat4 m_viewMatrix; // 视图矩阵 mat4 m_projMatrix; // 投影矩阵 void updateViewMatrix(); // 更新视图矩阵&#125;; 2. Camera 类实现123456789101112131415161718192021222324252627282930313233343536// camera.cpp#include &quot;core/camera.h&quot;Camera::Camera(const vec3f&amp; position, const vec3f&amp; target, const vec3f&amp; up) : m_position(position), m_target(target), m_up(up) &#123; updateViewMatrix(); m_projMatrix = mat4::identity();&#125;void Camera::setPerspective(float fovDegrees, float aspectRatio, float near, float far) &#123; m_projMatrix = mat4::perspective(fovDegrees * 3.1415926f / 180.0f, aspectRatio, near, far);&#125;mat4 Camera::getMVP(const mat4&amp; modelMatrix) const &#123; return m_projMatrix * m_viewMatrix * modelMatrix;&#125;void Camera::setPosition(const vec3f&amp; position) &#123; m_position = position; updateViewMatrix();&#125;void Camera::updateViewMatrix() &#123; vec3f forward = (m_target - m_position).normalized(); vec3f right = forward.cross(m_up).normalized(); vec3f up = right.cross(forward).normalized(); mat4 rotation; rotation.m[0][0] = right.x; rotation.m[0][1] = right.y; rotation.m[0][2] = right.z; rotation.m[0][3] = 0; rotation.m[1][0] = up.x; rotation.m[1][1] = up.y; rotation.m[1][2] = up.z; rotation.m[1][3] = 0; rotation.m[2][0] = -forward.x; rotation.m[2][1] = -forward.y; rotation.m[2][2] = -forward.z; rotation.m[2][3] = 0; rotation.m[3][0] = 0; rotation.m[3][1] = 0; rotation.m[3][2] = 0; rotation.m[3][3] = 1; mat4 translation = mat4::translation(-m_position.x, -m_position.y, -m_position.z); m_viewMatrix = rotation * translation;&#125; 3. 主循环集成12345678910111213141516171819202122232425262728293031// main.cppint main() &#123; const int width = 800, height = 800; Framebuffer framebuffer(width, height); framebuffer.clear(vec3f(0.5f, 0.5f, 0.5f)); framebuffer.clearZBuffer(); Model model; if (!model.loadFromObj(&quot;resources/obj/african_head.obj&quot;) || !model.loadDiffuseTexture(&quot;resources/diffuse/african_head_diffuse.tga&quot;)) &#123; std::cerr &lt;&lt; &quot;Failed to load model or texture&quot; &lt;&lt; std::endl; return 1; &#125; float near = 0.1f, far = 100.0f; Camera camera(vec3f(0, 0, 3), vec3f(0, 0, 0), vec3f(0, 1, 0)); camera.setPerspective(45.0f, (float)width / height, near, far); mat4 modelMatrix = mat4::identity(); mat4 mvp = camera.getMVP(modelMatrix); model.renderSolid(framebuffer, near, far, mvp, vec3f(1.0f, 1.0f, 1.0f), vec3f(0.0f, 0.0f, -1.0f)); framebuffer.flipVertical(); if (!framebuffer.saveToTGA(&quot;output.tga&quot;)) &#123; std::cerr &lt;&lt; &quot;Failed to save image&quot; &lt;&lt; std::endl; return 1; &#125; std::cout &lt;&lt; &quot;Rendered image saved to output.tga&quot; &lt;&lt; std::endl; return 0;&#125; 技术要点 坐标系：采用右手坐标系，+Z 为屏幕外，相机默认朝向由目标点决定。 视图矩阵：通过 lookAt 方法生成，先平移到相机原点，再旋转到相机坐标系。 退化处理：当 forward 和 up 平行时，需调整 up（如从 +Y 看 -Y 时用 -Z）。 投影矩阵：支持透视投影，FOV 转换为弧度，确保与 OpenGL 一致。 应用与验证应用场景 正面视角：相机位于 (0, 0, 3)，朝向 (0, 0, 0)，光照从 +Z 到 -Z，看到 african_head.obj 正面。 灵活调整：通过 setPosition 和目标点调整相机位置和朝向。 验证方法 正面验证： 配置：Camera(vec3f(0, 0, 3), vec3f(0, 0, 0), vec3f(0, 1, 0)) 光照：(0, 0, -1) 预期：看到模型正面。 背面验证： 配置：Camera(vec3f(0, 0, -3), vec3f(0, 0, 0), vec3f(0, 1, 0)) 光照：(0, 0, 1) 预期：看到模型背面。 侧面验证： 配置：Camera(vec3f(3, 0, 0), vec3f(0, 0, 0), vec3f(0, 1, 0)) 光照：(-1, 0, 0) 预期：看到模型右侧。 顶部验证： 配置：Camera(vec3f(0, 3, 0), vec3f(0, 0, 0), vec3f(0, 0, -1)) 光照：(0, -1, 0) 预期：看到模型顶部，无退化。 左前方验证： 配置：Camera(vec3f(-2, 0, 3), vec3f(0, 0, 0), vec3f(0, 1, 0)) 光照：(0.707, 0, -0.707) 预期：看到模型左前方。 总结通过实现 Camera 类，我们成功支持了灵活的相机控制，能够正确渲染 african_head.obj 的各个角度。验证过程确认了视图矩阵、光照和坐标系的一致性，确保了渲染结果符合预期。","tags":["Rendering","Camera System","View Matrix"],"categories":["Computer Graphics"]},{"title":"Diffuse Texture Loading Implementation","path":"/2025/04/04/DIFFUSE-TEXTURE/","content":"概述本文档详细记录了在 SoftRasterizer 项目中实现 diffuse 材质加载的全过程，解决了初始加载失败的问题，使得模型能够正确显示纹理效果。 核心修改1. Texture 类扩展1234567891011121314151617181920212223242526272829303132333435// 定义 Texture 类支持 TGA 加载class Texture &#123;public: int width = 0; int height = 0; std::vector&lt;vec3f&gt; pixels; bool loadFromTGA(const std::string&amp; filename); vec3f sample(float u, float v) const; bool empty() const &#123; return pixels.empty() || width == 0 || height == 0; &#125;&#125;;// 实现 TGA 文件加载bool Texture::loadFromTGA(const std::string&amp; filename) &#123; std::vector&lt;unsigned char&gt; data; if (!loadTGA(filename, width, height, data)) &#123; return false; &#125; pixels.resize(width * height); for (int y = 0; y &lt; height; y++) &#123; for (int x = 0; x &lt; width; x++) &#123; int idx = (y * width + x) * 3; pixels[y * width + x] = vec3f( data[idx] / 255.0f, // R data[idx + 1] / 255.0f, // G data[idx + 2] / 255.0f // B ); &#125; &#125; return true;&#125; 2. 支持 RLE 压缩的 TGA 加载12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758bool loadTGA(const std::string&amp; filename, int&amp; width, int&amp; height, std::vector&lt;unsigned char&gt;&amp; data) &#123; std::ifstream file(filename, std::ios::binary); if (!file.is_open()) return false; TGAHeader header; file.read(reinterpret_cast&lt;char*&gt;(&amp;header), sizeof(header)); // 支持未压缩 (2) 和 RLE 压缩 (10) 的 24 位 RGB 图像 if ((header.datatypecode != 2 &amp;&amp; header.datatypecode != 10) || header.bitsperpixel != 24) &#123; std::cerr &lt;&lt; &quot;Unsupported TGA format&quot; &lt;&lt; std::endl; return false; &#125; width = header.width; height = header.height; data.resize(width * height * 3); file.seekg(header.idlength + header.colormaplength * (header.colormapdepth / 8), std::ios::cur); if (header.datatypecode == 2) &#123; file.read(reinterpret_cast&lt;char*&gt;(data.data()), data.size()); &#125; else if (header.datatypecode == 10) &#123; size_t pixelCount = width * height; size_t currentPixel = 0; unsigned char pixel[3]; while (currentPixel &lt; pixelCount) &#123; unsigned char chunkHeader; file.read(reinterpret_cast&lt;char*&gt;(&amp;chunkHeader), 1); if (chunkHeader &lt; 128) &#123; // Raw packet size_t count = chunkHeader + 1; for (size_t i = 0; i &lt; count &amp;&amp; currentPixel &lt; pixelCount; ++i) &#123; file.read(reinterpret_cast&lt;char*&gt;(pixel), 3); data[currentPixel * 3] = pixel[0]; data[currentPixel * 3 + 1] = pixel[1]; data[currentPixel * 3 + 2] = pixel[2]; currentPixel++; &#125; &#125; else &#123; // RLE packet size_t count = chunkHeader - 127; file.read(reinterpret_cast&lt;char*&gt;(pixel), 3); for (size_t i = 0; i &lt; count &amp;&amp; currentPixel &lt; pixelCount; ++i) &#123; data[currentPixel * 3] = pixel[0]; data[currentPixel * 3 + 1] = pixel[1]; data[currentPixel * 3 + 2] = pixel[2]; currentPixel++; &#125; &#125; &#125; &#125; // BGR 转 RGB for (size_t i = 0; i &lt; data.size(); i += 3) &#123; std::swap(data[i], data[i + 2]); &#125; return true;&#125; 3. Model 类集成123456789101112class Model &#123;public: Texture diffuseTexture; bool loadDiffuseTexture(const std::string&amp; filename) &#123; return diffuseTexture.loadFromTGA(filename); &#125; void renderSolid(Framebuffer&amp; fb, const vec3f&amp; lightDir, const vec3f&amp; eye) &#123; // 使用 diffuseTexture 进行渲染... &#125;&#125;; 4. 主程序调整12345678910111213141516171819202122int main() &#123; Framebuffer framebuffer(800, 800); framebuffer.clear(vec3f(0.5f, 0.5f, 0.5f)); framebuffer.clearZBuffer(); Model model; if (!model.loadFromObj(&quot;resources/obj/african_head.obj&quot;)) &#123; std::cerr &lt;&lt; &quot;Failed to load model&quot; &lt;&lt; std::endl; return 1; &#125; if (!model.loadDiffuseTexture(&quot;resources/diffuse/african_head_diffuse.tga&quot;)) &#123; std::cerr &lt;&lt; &quot;Failed to load texture&quot; &lt;&lt; std::endl; return 1; &#125; model.renderSolid(framebuffer, vec3f(1.0f, 1.0f, 1.0f), vec3f(0.0f, 0.0f, 1.0f)); framebuffer.flipVertical(); framebuffer.saveToTGA(&quot;output.tga&quot;); return 0;&#125; 技术要点 TGA 格式支持：扩展 loadTGA 函数，支持 datatypecode &#x3D;&#x3D; 2（未压缩 RGB）和 datatypecode &#x3D;&#x3D; 10（RLE 压缩 RGB）。 RLE 解码：实现 RLE 压缩的解码逻辑，处理 raw 和 RLE 数据包。 颜色转换：将 TGA 文件的 BGR 格式转换为 RGB 格式。 错误处理：添加详细的调试输出，确保加载失败时能定位问题。 验证方法 检查输出图像 output.tga，确认模型表面显示正确的 diffuse 纹理。 验证纹理坐标 (u, v) 的插值是否正确，纹理无拉伸或错位。 确保 RLE 压缩的 TGA 文件能够正常加载并渲染。 检查程序运行时无 “Failed to load texture” 错误输出。 修改过程回顾最初，程序因 “Failed to load texture” 而失败，原因是 african_head_diffuse.tga 文件使用了 RLE 压缩（datatypecode &#x3D;&#x3D; 10），而原始代码只支持未压缩格式（datatypecode &#x3D;&#x3D; 2）。通过调试输出确认问题后，我扩展了 loadTGA 函数，添加了对 RLE 压缩的支持，最终成功加载并渲染了 diffuse 材质。","tags":["Rendering","Texture Mapping"],"categories":["Computer Graphics"]},{"title":"Z-Buffer Implementation","path":"/2025/04/04/Z-BUFFER-IMPLEMENTATION/","content":"概述本文档详细记录了在SoftRasterizer项目中实现Z-Buffer深度测试的全过程。 核心修改1. 帧缓冲类改造1234567891011121314// 添加深度缓冲区std::vector&lt;float&gt; zBuffer;// 初始化Framebuffer::Framebuffer(int w, int h) : width(w), height(h), pixels(w * h), zBuffer(w * h, std::numeric_limits&lt;float&gt;::lowest()) &#123;&#125;// 清空深度缓冲void clearZBuffer() &#123; std::fill(zBuffer.begin(), zBuffer.end(), std::numeric_limits&lt;float&gt;::lowest());&#125; 2. 深度测试实现12345678910void setPixel(int x, int y, const vec3f&amp; color, float depth) &#123; if (x &gt;= 0 &amp;&amp; x &lt; width &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; height) &#123; int index = y * width + x; // 右手坐标系：z值越大表示越远 if (depth &gt; zBuffer[index]) &#123; zBuffer[index] = depth; pixels[index] = color; &#125; &#125;&#125; 3. 三角形渲染优化12345678910111213141516void drawTriangle(/* 参数 */) &#123; // 顶点排序和退化检测... // 顶部渲染 for (int y = y0; y &lt;= y1; y++) &#123; // 边界检查 if (y &lt; 0 || y &gt;= height) continue; // 坐标插值 int xa = interpolate(x0, y0, x2, y2, y); int xb = interpolate(x0, y0, x1, y1, y); float za = (y2 != y0) ? z0 + (z2 - z0) * (y - y0) / (y2 - y0) : z0; // ...其余代码 &#125; // 底部渲染类似...&#125; 4. 主循环集成123// 每帧清空framebuffer.clear(vec3f(0.1f, 0.1f, 0.1f));framebuffer.clearZBuffer(); 技术要点 坐标系：采用右手坐标系，+Z指向观察者后方 深度比较：使用&gt;运算符进行深度测试 初始值：使用lowest()而非min() 边界处理：完善的越界检查和除零保护 验证方法 近处物体正确遮挡远处物体 无三角形破碎现象 表面深度过渡平滑 无闪烁或Z-fighting现象","tags":["Rendering","Depth Buffer"],"categories":["Computer Graphics"]},{"title":"OBJ模型加载与三角形渲染实现","path":"/2025/04/03/OBJ-RENDERING-IMPLEMENTATION/","content":"OBJ模型加载与三角形渲染实现坐标系确定本渲染器使用左手坐标系，判断依据： 静态分析方法 检查顶点变换： 123// 没有Z轴反转操作，保持原始方向screen_coords[j] = vec2i((v.x+1)*fb.width/2, (v.y+1)*fb.height/2);// Z值保持不变，直接用于深度比较 检查法线计算： 12vec3f normal = (v2-v0).cross(v1-v0).normalized();// 叉乘顺序决定法线方向，与左手系一致 检查光照计算： 12float intensity = normal.dot(lightDir.normalized());// 当lightDir=(0,0,1)时，朝前的面(intensity&gt;0)会被渲染 动态验证方法 创建测试三角形： 1vertices = &#123;&#123;0,1,0&#125;, &#123;-1,-1,0&#125;, &#123;1,-1,0&#125;&#125;; // 朝向+z 观察不同光照方向效果： lightDir(0,0,1) 应可见 lightDir(0,0,-1) 应不可见 实现概述本次实现了OBJ模型加载和三角形渲染功能，主要包含： OBJ文件格式解析 三角形面片渲染 基础光照计算 背面剔除优化 核心实现1. OBJ文件加载123456789101112131415161718bool Model::loadFromObj(const std::string&amp; filename) &#123; // 解析顶点数据 if (type == &quot;v&quot;) &#123; vec3f v; iss &gt;&gt; v.x &gt;&gt; v.y &gt;&gt; v.z; vertices.push_back(v); &#125; // 解析面数据 else if (type == &quot;f&quot;) &#123; // 处理v/vt/vn等多种格式 while (iss &gt;&gt; v) &#123; face.push_back(v - 1); // OBJ使用1-based索引 if (iss.peek() == &#x27;/&#x27;) &#123; // 处理纹理/法线坐标... &#125; &#125; &#125;&#125; 2. 三角形渲染与光照1234567891011121314void Model::renderSolid(Framebuffer&amp; fb, const vec3f&amp; color, const vec3f&amp; lightDir) &#123; // 计算面法线 vec3f normal = calculateFaceNormal(face); // 光照计算（Lambert模型） float intensity = normal.dot(lightDir.normalized()); if (intensity &gt; 0) &#123; // 背面剔除 vec3f shadedColor = color * intensity; // 三角形光栅化 fb.drawTriangle(x0,y0, x1,y1, x2,y2, shadedColor); &#125;&#125; 3. 法线计算12345vec3f Model::calculateFaceNormal() const &#123; vec3f edge1 = v1 - v0; vec3f edge2 = v2 - v0; return edge1.cross(edge2).normalized();&#125; 关键技术点 OBJ格式解析： 支持顶点&#x2F;纹理&#x2F;法线坐标 处理多种面定义格式(v, v&#x2F;vt, v&#x2F;&#x2F;vn, v&#x2F;vt&#x2F;vn) 1-based到0-based索引转换 渲染优化： 背面剔除：跳过dot product ≤ 0的面 扫描线算法：高效三角形填充 法线插值：使用顶点法线或几何法线 光照模型： 简单Lambert漫反射 光线方向归一化处理 颜色强度线性缩放 使用方法12345678Model model;model.loadFromObj(&quot;model.obj&quot;);// 设置光照方向(指向屏幕里)vec3f lightDir(0,0,1); // 渲染模型(白色)model.renderSolid(fb, vec3f(1,1,1), lightDir); 效果验证渲染测试模型后应得到： 正确朝向的面片被渲染 背对光源的面片被剔除 光照强度随角度变化 后续计划 实现Z-buffer深度测试 添加纹理映射支持 实现Phong光照模型","tags":["计算机图形学","C++","模型渲染"],"categories":["技术实践"]},{"title":"软光栅渲染器开发记录","path":"/2025/04/01/SOFTRASTERIZER-INTRODUCTION/","content":"软光栅渲染器开发阶段性成果项目概述我们实现了一个基础的软光栅渲染器，具有以下特点： 完全从零实现，不依赖图形API 仅使用标准库和基础数学运算 支持基本的像素绘制和图像输出 核心功能实现1. 数学库123456789101112// 向量模板类template&lt;typename T&gt;struct Vector3 &#123; T x, y, z; // 向量运算...&#125;;// 4x4矩阵struct mat4 &#123; float m[4][4]; // 矩阵运算和变换...&#125;; 2. 帧缓冲管理12345678class Framebuffer &#123; int width, height; std::vector&lt;vec3f&gt; pixels; public: // 清屏、像素绘制等操作... bool saveToTGA(const std::string&amp; filename);&#125;; 3. TGA图像输出实现了Truevision TGA格式的图像输出： 支持24位RGB格式 包含完整的文件头结构 像素数据BGR排列 项目结构12345678SoftRasterizer/├── include/│ ├── math/ # 数学库│ └── core/ # 核心渲染组件├── src/│ ├── io/ # 文件IO│ └── core/ # 实现代码└── CMakeLists.txt # 构建配置 使用方法 构建项目： 12cmake -S . -B buildcmake --build build --config Release 运行程序： 1./build/Release/SoftRasterizer.exe 后续计划 实现OBJ模型加载 添加三角形光栅化 支持深度缓冲(Z-buffer) 实现基础光照模型 查看完整代码","tags":["计算机图形学","C++","渲染引擎"],"categories":["技术实践"]},{"title":"软光栅直线绘制算法实现","path":"/2025/04/01/LINE-DRAWING-ALGORITHM/","content":"直线光栅化基础算法 - Bresenham实现算法简介Bresenham算法是计算机图形学中最基础的直线光栅化算法，通过整数运算高效确定最佳逼近直线路径的像素点。 核心特点 完全整数运算，无浮点计算 避免乘除法，仅用加减和位运算 一次生成一个像素，时间复杂度O(n) 实现原理基本思想算法通过误差项决定下一个像素的选择： 以x为步进方向 计算Δy&#x2F;Δx的斜率 维护误差项跟踪实际直线与像素中心的距离 根据误差决定是否增加&#x2F;decrease y 关键优化1234567bool steep = abs(y1 - y0) &gt; abs(x1 - x0); // 是否为陡峭线if (steep) std::swap(x0, y0); // 统一处理为缓变线if (x0 &gt; x1) std::swap(x0, x1); // 确保从左到右绘制int dx = x1 - x0;int dy = abs(y1 - y0);int err = dx / 2; // 初始误差 接口实现添加到Framebuffer类： 12345class Framebuffer &#123;public: // ... void drawLine(int x0, int y0, int x1, int y1, const vec3f&amp; color);&#125;; 测试用例测试不同方向的直线绘制： 12345678// 水平线（红色）framebuffer.drawLine(100, 100, 700, 100, vec3f(1,0,0));// 垂直线（蓝色） framebuffer.drawLine(400, 100, 400, 500, vec3f(0,0,1));// 对角线（绿色）framebuffer.drawLine(100, 150, 700, 500, vec3f(0,1,0)); 效果验证生成图像应包含： 正确朝向的3D模型线框 所有边线完整连接 无断裂或缺失像素 坐标系统说明模型渲染时进行了坐标转换： X坐标：保持原样 (x1 &#x3D; (v1.x + 1) * width &#x2F; 2) Y坐标：翻转以符合屏幕坐标系 (y1 &#x3D; height - (v1.y + 1) * height &#x2F; 2) Z坐标：暂时忽略 继续阅读 Bresenham原始论文 算法优化技巧 返回项目主页","tags":["计算机图形学","C++","渲染引擎"],"categories":["技术实践"]},{"title":"计算机图形学——第2章：图形系统","path":"/2025/03/22/计算机图形学——第2章：图形系统/","content":"计算机图形学：第2章 图形系统使用计算机进行图形处理时，需要有一个由硬件和软件组成的计算机图形系统，也就是我们所说的支撑环境。本章主要讨论计算机图形系统完成图形显示任务的原理和方式，并且对图形系统所涉及的主要软件和硬件进行必要的介绍。最后对图形流水线进行介绍和分析。 2.1 图形系统概述 2.1.1 图形硬件图形显示设备用于观察，修改图形，它是人机交互式处理图形的有力工具。 图形绘制设备是用于输出图形到介质的设备。可分为光栅点阵型（打印机）和随机矢量型（笔试绘图仪）。 2.1.2 图形软件广义上的图形程序。可分为图形应用软件、图形支撑软件和图形应用数据结构3部分。 若以Pascal语言之父提出的公式“程序&#x3D;算法+数据结构”来类比，则有 图形程序=图形算法+图形应用数据结构 2.2 图形硬件2.2.1 图形显示设备 阴极射线管 液晶显示器 2.2.2 图形显示方式 随机扫描显示 光栅扫描显示 2.2.3 光栅扫描显示系统在此系统中，电子束横向扫描屏幕，从左到右，从上到下，一次一行顺次进行。当电子束横向沿每一行移动时，电子束的强度不断变化来建立亮点的图案，构成图像并显示在屏幕上。 光栅扫描显示系统的组成3部分： 显示器、视频控制器和帧缓冲存储器。其中，显示器屏幕图形是依靠帧缓冲进行刷新的，而视频控制器是负责刷新的部件。目前常见的光栅显示器主要有彩色阴极射线管与液晶显示器两种。 光栅扫描显示系统的结构 2.2.4 显卡和图形处理器 显卡显卡(Video Card, Graphics Card)又称显示接口卡，也称显示适配器。它是主机与显卡之间的桥梁，控制计算机图形输出，负责将CPU送来的图像数据处理成显示器接受的格式，再送到显示器形成图像。显卡各部分组成及其与周边设备的关系如图所示： 2.3 图形软件2.4 图形流水线2.4.1 图形流水线三阶段 应用程序阶段一般将数据以图元的形式提供给图形硬件，如用来描述三维几何模型的点、线或多边形。同时也提供用于表面纹理映射的图像或位图。 几何处理阶段是以每个顶点为基础对几何图元进行处理，并从三维坐标变换到二维屏幕坐标的过程。该阶段在GPU上进行。目标是确定哪些几何图像可以在屏幕上显示，并把颜色值赋给这些对象的顶点。可以进一步划分为顶点变换、投影、裁剪、顶点着色等阶段。 光栅阶段，屏幕对象首先被传送到像素处理器进行光栅化，并对每个像素进行着色，然后输出到显示器。目的就是给像素准确配色，正确绘制整幅图像。此过程称为光栅化或扫描转换。 2.4.2 图形流水线关键步骤"},{"title":"计算机图形学——第1章：绪论","path":"/2025/03/22/计算机图形学——第1章：绪论/","content":"计算机图形学：第1章 绪论 “图形是人类与计算机对话的窗口，而计算机图形学则是打开这扇窗的钥匙。” 计算机图形学（Computer Graphics）是一门研究如何利用计算机生成、处理和显示图形的学科。它不仅是计算机科学的重要分支，还融合了数学、物理学和艺术的精髓。从最初的简单线框图到如今的实时光线追踪，计算机图形学已经深刻改变了我们的生活方式。 本章将带你走进计算机图形学的世界，探索其定义、内涵以及发展历程。我们将从 4W 问题（What, Why, Where, When）入手，逐步揭开图形学的神秘面纱。 1.1 计算机图形学的定义与内涵在学习计算机图形学之前，我们需要明确它的定义和研究对象。简单来说，计算机图形学是一门研究如何通过计算机生成和处理图形的学科。它不仅关注图像的生成，还涉及如何让这些图像更逼真、更高效地呈现。 定义计算机图形学的核心在于 “形” 和 “光”： 形：指几何形状的建模与表示，例如如何用数学方法描述一个三维物体。 光：指光照效果的模拟，例如如何通过算法计算光线与物体的交互。 内涵计算机图形学的研究内容可以从以下几个方面展开： 建模：如何用数学方法描述三维物体？ 渲染：如何将三维模型转化为二维图像？ 动画：如何让静态物体动起来？ 交互：如何实现用户与图形的实时互动？ 通过这些研究，计算机图形学能够将抽象的数学模型转化为直观的视觉效果，为用户提供沉浸式的体验。 1.2 图形及其与图像的区别在学习图形学时，我们常常会遇到“图形”和“图像”这两个概念。它们看似相似，但实际上有着本质的区别。 1. 图形我们生活在一个充满图形的现实世界中。无论是自然界的花草树木，还是人造的建筑车辆，这些物体都可以被抽象为 “形”。在计算机图形学中，图形是指通过数学模型描述的几何形状，例如点、线、面等。 特点：图形是 矢量化的，可以无损缩放。 应用：CAD 设计、游戏建模。 2. 图像图像则是对现实世界的采样结果，通常以像素的形式存储。无论是照片、视频还是屏幕上的显示内容，图像都是由像素点组成的。 特点：图像是 光栅化的，缩放可能会失真。 应用：数字摄影、图像处理。 3. 图形与图像的关系 从“形”到“图”：通过渲染技术，图形可以转化为图像。例如，一个三维模型经过光照计算后，生成一张二维图片。 从“图”到“形”：通过逆向工程，图像可以重建为图形。例如，通过图像识别技术提取物体的轮廓。 4. 图形与图像的关系以下表格从多个维度对比 图形 和 图像 的特点： 维度 图形 图像 定义 通过数学模型描述的几何形状，通常以矢量形式存储（如点、线、面）。 对现实世界的采样结果，以像素（光栅化）形式存储（如照片）。 存储方式 以数学公式或向量数据存储（如 SVG 文件），文件体积小。 以像素网格存储（如 PNG、JPEG 文件），文件体积较大。 缩放效果 支持无损缩放，放大后不会失真。 缩放可能导致失真，放大后会出现像素化（锯齿）。 生成方式 通过算法生成，通常由建模和渲染技术创建。 通过设备采样（如相机拍摄）或渲染图形后生成。 编辑方式 直接修改几何属性（如调整坐标、形状），编辑灵活。 通过图像处理软件（如 Photoshop）编辑像素，修改复杂。 应用场景 CAD 设计、游戏 3D 模型、矢量插图等需要精确建模的领域。 数字摄影、视频帧、网页图片等视觉呈现场景。 与计算机图形学的关系 核心研究对象，关注生成和操作几何形状。 图形的渲染结果，通过渲染技术由图形转化而来。 转化关系 可通过渲染技术（如光栅化）转化为图像。 可通过逆向工程（如图像识别）提取图形信息。 5. 小结 图形 更注重数学描述和可编辑性，是计算机图形学的起点。 图像 更注重视觉呈现和直观性，是图形的最终输出形式。 1.3 计算机图形学的 4W 问题为了更全面地理解计算机图形学，我们可以从以下 4W 问题入手： What（是什么）计算机图形学是一门研究如何生成和处理图形的学科，核心在于“形”与“光”的结合。 Why（为什么学）图形学不仅是技术，更是艺术与科学的结合。它推动了游戏、电影、虚拟现实等行业的发展，为人类提供了更直观的表达方式。 Where（应用在哪里）计算机图形学的应用无处不在： 娱乐：电影特效（如《阿凡达》）、游戏渲染（如《赛博朋克2077》）。 科学：医学成像、气象模拟。 工业：建筑设计、汽车建模。 When（什么时候学）学习计算机图形学需要一定的数学基础（如线性代数、微积分）和编程能力（如 C++ 或 Python）。建议在掌握这些基础后开始学习。 1.4 本章小结本章作为计算机图形学的开篇，介绍了其定义、内涵以及图形与图像的区别。通过 4W 问题，我们初步了解了图形学的核心内容和应用场景。下一章，我们将深入探讨图形学的基础数学工具，为后续学习打下坚实基础。 “图形学的魅力在于，它不仅让我们看到世界，还让我们创造世界。”","tags":["计算机图形学","基础知识","绪论"]},{"title":"Hello World","path":"/2025/03/20/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]